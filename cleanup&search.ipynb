{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "5e25a538-7eca-4ca0-80d1-e8956095628d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting de-core-news-lg==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.4.0/de_core_news_lg-3.4.0-py3-none-any.whl (567.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 567.8/567.8 MB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from de-core-news-lg==3.4.0) (3.4.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (8.1.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (0.8.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (1.23.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (1.10.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/jupyterhub/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/jupyterhub/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/jupyterhub/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/jupyterhub/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/jupyterhub/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/jupyterhub/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (2022.9.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/jupyterhub/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/jupyterhub/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/jupyterhub/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/jupyterhub/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-lg==3.4.0) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 23.3.2\n",
      "[notice] To update, run: python3.10 -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/tianyi.zhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the relevant packages\n",
    "import re\n",
    "import pandas as pd\n",
    "#tokenizer, tagger, ngrams, porterstemmer, wordnetlemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, bigrams, ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import *\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"de_core_news_lg\")\n",
    "nlp = spacy.load('de_core_news_lg')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "525aef49-fa0b-43fa-9246-bbe4f9a8502b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get rid of the numbering (+ .)\n",
    "numbering = re.compile(r\"s*^\\d+(?:\\d+)+\\s*\")\n",
    "\n",
    "with open(\"all_corpora_9522.txt\", \"r\") as file, open(\"all_corpora_9522.preprocessed1\", \"w\") as fout:\n",
    "    for line in file:\n",
    "        new_line = numbering.sub(\"\", line)\n",
    "        print(new_line, file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "e222b23e-c52f-493e-b513-b99c93f250c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Küchenkünstler Fouquet Yorker Boite Bois verantwortlich gute Zeit besten Pariser Hotel George arbeitete Brigade Berliners Siggi Rockendorf\\n', '\\n', 'empfängt Besucher liebsten Miroslaw Fiedorowicz Kollege Zgorzelec neben sitzt\\n', '\\n', 'Hoffentlich eröffnet Berliner Finanzsenator Ausstellung\\n', '\\n', 'Besucher rührende Zutraulichkeit erfahren Amerikaner wildfremde Gäste aufnehmen Gestrandeten weiterhelfen Innerstes preisgeben\\n', '\\n', 'Kapitän Ricard Persson erhöhte weiteren Berliner Überzahlspiel sogar Distanzschuss Team Geschehen bestimmte\\n', '\\n', 'Bischofsheim August Bischofsheimer Friedhof sticht Besuchern silbrig glänzende Pyramide Auge Stelen umgeben\\n', '\\n', 'Familie Prinzessin Diana Jahr erste offizielle Biografie Königin Herzen veröffentlichen\\n', '\\n', 'Übernahme Jüdischen Museums Berliner Festspiele Hauses Kulturen Welt MartinGropiusBaus kamen Einrichtungen nationaler Bedeutung systematisch Obhut Bundes\\n', '\\n', 'Besucher Halle Obergeschoss angeht sitzen Rücken Vorhang klagen eben selten darüber Stelle empfindlich zieht\\n', '\\n', 'Eltern Geschwister Therapie einbezogen\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# check if the cleaning worked\n",
    "# print the first few lines\n",
    "with open(\"all_corpora_9522_targets.preprocessed2\") as check_file:\n",
    "    head = [next(check_file) for _ in range(20)]\n",
    "print(head)\n",
    "\n",
    "# sample random few lines\n",
    "import random\n",
    "with open('all_corpora_9522_targets.preprocessed2') as f:\n",
    "    lines = random.sample(f.readlines(),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "94939f07-5238-4588-b57d-714c26dde698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Get rid of the punctuation\n",
    "import string\n",
    "punct = string.punctuation\n",
    "print(punct)\n",
    "\n",
    "pattern = re.compile(r\"([\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~\\“\\„\\«\\–\\»])\")\n",
    "\n",
    "with open(\"all_corpora_9522.preprocessed1\", \"r\") as file, open(\"all_corpora_9522.preprocessed2\", \"w\") as fout:\n",
    "    for line in file:\n",
    "        new_line = pattern.sub(r\"\", line)\n",
    "        print(new_line, file  = fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "8b75c2a3-4343-4ef5-a04f-54076eeb1917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get rid of the extra blank lines\n",
    "with open('all_corpora_9522.preprocessed2') as reader, open('all_corpora_9522.preprocessed2', 'r+') as writer:\n",
    "    for line in reader:\n",
    "        if line.strip():\n",
    "            writer.write(line)\n",
    "    writer.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "efacb8fe-9764-4a7d-af37-cc5edb1ffa64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract only the lines which contain the target words from the word list\n",
    "\n",
    "wordlist = [\"Bewohner\", \"Propheten\", \"Berliner\", \"Bewerber\", \"Besitzer\", \"Pariser\", \"Banditen\", \"Blondinen\", \"Germanen\",\n",
    "\"Geschwister\", \"Gemahlin\", \"Gewinner\", \"Prinzessin\", \"Professor\", \"Piraten\", \"Piloten\", \"Touristen\", \"Dozenten\", \"Tyrannen\", \n",
    "\"Kollegen\", \"Kumpanen\", \"Besucher\"]\n",
    "\n",
    "with open(\"all_corpora_9522.preprocessed2\", \"r\") as file, open(\"all_corpora_9522_targets.txt\", \"w\") as fout:\n",
    "    sentences = file.read().splitlines()\n",
    "    for sentence in sentences:\n",
    "        if any(i in sentence for i in wordlist):\n",
    "            print(sentence, file = fout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "4b842d37-0a95-4bce-b022-2454a5609aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "\n",
    "stopwords = set(stopwords.words('german'))\n",
    "\n",
    "with open(\"all_corpora_9522_targets.txt\",\"r\") as file, open(\"all_corpora_9522_targets.preprocessed\",\"w\") as fout: \n",
    "    for line in file.readlines(): \n",
    "        print(\" \".join([word for word in line.translate(str.maketrans('', '', string.punctuation)).split()  \n",
    "            if len(word) >=4 and word.lower() not in stopwords]), file = fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "792f7d64-11c2-4026-be7b-92fbc37f96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers in each line\n",
    "\n",
    "with open(\"all_corpora_9522_targets.preprocessed\", \"r\") as file, open(\"all_corpora_9522_targets.preprocessed2\", \"w\") as fout:\n",
    "    for line in file:\n",
    "        # \\b: Match word boundary\n",
    "        # (?:\\d+|\\w): Match a single word character or 1+ digits\n",
    "        # \\b: Match word boundary\n",
    "        # \\s*: Match 0 or more whitespaces\n",
    "        new_line = re.sub(r'\\b(?:\\d+|\\w)\\b\\s*', '', line)\n",
    "        print(new_line, file  = fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f7a5e86d-4069-4d5a-b2af-7b09c4295c57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lemmatize each word in the remaining part of the sentence\n",
    "# Left out for now\n",
    "\n",
    "fout = \"2011_mixed_sentences.preprocessed3\"\n",
    "\n",
    "with open(\"2011_mixed_sentences.preprocessed2\", \"r\") as file, open(fout, \"w\") as fout:\n",
    "\n",
    "    content = file.read() # read all content of the file\n",
    "    sentences = sent_tokenize(content) # tokenize the content into sentences\n",
    "    \n",
    "    for sentence in sentences: \n",
    "        # lemmatize with spacy       \n",
    "        sentence = nlp(sentence) # pass the text to the spacy pipeline\n",
    "        lem = [w.lemma_ for w in sentence] # lemmatize the words in the preprocessed text\n",
    "        \n",
    "        for i in range(len(lem)): # loop through the lemma list and lowercase each word\n",
    "            lem[i] = lem[i].lower() \n",
    "        \n",
    "        lemmas = ' '.join(lem) # get the lemmas back into one string again, each separated by white space\n",
    "\n",
    "        print(lemmas, file = fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2eac6475-2d90-41fe-8f31-ef2276088a09",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Besucher', 'erwartet'), 37), (('Berliner', 'Zeitung'), 32), (('Berliner', 'Polizei'), 23), (('viele', 'Besucher'), 22), (('Besucher', 'seit'), 19), (('Berliner', 'Senat'), 19), (('Berliner', 'Landgericht'), 18), (('kleinen', 'Besucher'), 16), (('erwartet', 'Besucher'), 16), (('Auch', 'Berliner'), 16), (('Veranstalter', 'erwarten'), 15), (('Besucher', 'kamen'), 14), (('Bewohner', 'wurden'), 13), (('wurde', 'Professor'), 12), (('neuen', 'Besitzer'), 12), (('rund', 'Besucher'), 12), (('teilte', 'Berliner'), 12), (('mehr', 'Besucher'), 11), (('Millionen', 'Besucher'), 11), (('Besucher', 'unserer'), 11), (('größten', 'Gewinnern'), 11), (('Gewinnern', 'zählten'), 11), (('wurde', 'Berliner'), 10), (('Gewinner', 'Verlierer'), 10), (('große', 'Gewinner'), 10), (('Seine', 'Kollegen'), 10), (('Berliner', 'Mauer'), 9), (('Besucher', 'gekommen'), 9), (('Besucher', 'konnten'), 9), (('Berliner', 'Morgenpost'), 9), (('Veranstalter', 'rechnen'), 9), (('Professor', 'ernannt'), 8), (('Berliner', 'Politik'), 8), (('Berliner', 'Senats'), 8), (('Viele', 'Besucher'), 8), (('Gewinnern', 'gehörten'), 8), (('viele', 'Touristen'), 7), (('Liebe', 'Besucher'), 7), (('Alle', 'Gewinner'), 7), (('Gewinner', 'wurden'), 7), (('Auch', 'Besucher'), 7), (('erwarten', 'Besucher'), 7), (('viele', 'Berliner'), 7), (('vergangenen', 'Jahr'), 7), (('Besucher', 'gezählt'), 7), (('Sicherheit', 'gebracht'), 7), (('Bericht', 'Berliner'), 7), (('1000', 'Besucher'), 6), (('bekannt', 'gegeben'), 6), (('Besucher', 'geöffnet'), 6)]\n"
     ]
    }
   ],
   "source": [
    "ngram_counts = Counter()\n",
    "with open('2011_mixed_sentences.preprocessed') as mixed2011:\n",
    "    for l in mixed2011:\n",
    "        ngram_counts.update(Counter(ngrams(l.split(), 2)))\n",
    "\n",
    "print(ngram_counts.most_common(50))      \n",
    "        \n",
    "        \n",
    "with open(\"ngrams.txt\", 'w') as f:\n",
    "    for k,v in ngram_counts.most_common():\n",
    "        f.write(\"{} {}\\n\".format(k,v))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "3486ef99-d5c3-4712-9f08-117d1cd26517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate lexical frequency of the keywords in the corpus\n",
    "#keyword_list = [\"bewohner\", \"propheten\", \"berliner\", \"bewerber\", \"besitzer\", \"pariser\", \"banditen\", \"blondinen\", \"germanen\",\n",
    "#\"geschwister\", \"gemahlin\", \"gewinner\", \"prinzessin\", \"professor\", \"piraten\", \"piloten\", \"touristen\", \"dozenten\", \"tyrannen\", \n",
    "#\"kollegen\", \"kumpanen\", \"besucher\"]\n",
    "\n",
    "keyword_list = [\"Bewohner\", \"Propheten\", \"Berliner\", \"Bewerber\", \"Besitzer\", \"Pariser\", \"Banditen\", \"Blondinen\", \"Germanen\",\n",
    "\"Geschwister\", \"Gemahlin\", \"Gewinner\", \"Prinzessin\", \"Professor\", \"Piraten\", \"Piloten\", \"Touristen\", \"Dozenten\", \"Tyrannen\", \n",
    "\"Kollegen\", \"Kumpanen\", \"Besucher\"]\n",
    "                \n",
    "corpus = \"all_corpora_9522_targets.preprocessed2\" \n",
    "\n",
    "def keyword_freq(keyword_list, corpus):\n",
    "    # Initialize a dictionary to store keyword frequency in the corpus\n",
    "    keyword_freq = {keyword: 0 for keyword in keyword_list}\n",
    "\n",
    "    with open(corpus, 'r') as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "\n",
    "            for keyword in keyword_list:\n",
    "                # count if the keyword is present in the line\n",
    "                if keyword in words:\n",
    "                    keyword_freq[keyword] += 1\n",
    "\n",
    "    return keyword_freq\n",
    "\n",
    "\n",
    "result = keyword_freq(keyword_list, corpus)\n",
    "\n",
    "with open(\"lex_freq.txt\", \"w\") as fout:\n",
    "    for keyword, count in result.items():\n",
    "        print(keyword, count, sep = \"\\t\", file = fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "2569267b-e915-485d-9806-d0826accf663",
   "metadata": {},
   "outputs": [],
   "source": [
    "prime_list = [\"Nachbarn\", \"Botschaft\", \"Brandenburger\", \"Vorstellungsgespräch\", \"Eigentum\", \"Eiffelturm\", \"Geld\", \"Haar\", \"Römer\",\n",
    "\"Bruder\", \"Ehe\", \"Gegner\", \"Königreich\", \"Vorlesung\", \"Frachtschiff\", \"Flugzeug\", \"Sehenswürdigkeit\", \"Universität\", \"Volk\", \n",
    "\"Chef\", \"Gefährten\", \"Eintritt\"]\n",
    "                \n",
    "corpus = \"all_corpora_9522_targets.preprocessed2\" \n",
    "output_file_path = \"prime_freq.txt\"\n",
    "\n",
    "def count_word_occurrences(words, text):\n",
    "    word_counts = {word.lower(): 0 for word in words}\n",
    "\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        count = text.lower().count(word_lower)\n",
    "        word_counts[word_lower] = count\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "def save_counts_to_file(word_counts, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(\"Word\\tCount\\n\")\n",
    "        for word, count in word_counts.items():\n",
    "            file.write(f\"{word}\\t{count}\\n\")\n",
    "\n",
    "# Read text from corpus file\n",
    "with open(corpus_file_path, 'r', encoding='utf-8') as file:\n",
    "    corpus_text = file.read()\n",
    "\n",
    "# Count occurrences of words in the text\n",
    "word_occurrences = count_word_occurrences(prime_list, corpus_text)\n",
    "\n",
    "# Save results to a text file\n",
    "save_counts_to_file(word_occurrences, output_file_path)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "e848c288-c420-4edd-8302-62d5c19c6582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the corpus: 5529689\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        tokens = text.split()\n",
    "        return len(tokens)\n",
    "\n",
    "# Example usage\n",
    "corpus_file_path = \"all_corpora_9522_targets.preprocessed2\"\n",
    "\n",
    "total_tokens = count_tokens(corpus_file_path)\n",
    "\n",
    "print(f\"Total number of tokens in the corpus: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "b7cce6c6-7242-4fdc-99d5-82398d5f52a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the frequency of words co-occurring with keywords and save the results\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Define the keyword list, the input and output file first\n",
    "keywords = [\"Bewohner\", \"Propheten\", \"Berliner\", \"Bewerber\", \"Besitzer\", \"Pariser\", \"Banditen\", \"Blondinen\", \"Germanen\",\n",
    "\"Geschwister\", \"Gemahlin\", \"Gewinner\", \"Prinzessin\", \"Professor\", \"Piraten\", \"Piloten\", \"Touristen\", \"Dozenten\", \"Tyrannen\", \n",
    "\"Kollegen\", \"Kumpanen\", \"Besucher\"]\n",
    "#keyword_list = [\"bewohner\", \"propheten\", \"berliner\", \"bewerber\", \"besitzer\", \"pariser\", \"banditen\", \"blondinen\", \"germanen\",\n",
    "#\"geschwister\", \"gemahlin\", \"gewinner\", \"prinzessin\", \"professor\", \"piraten\", \"piloten\", \"touristen\", \"dozenten\", \"tyrannen\", \n",
    "#\"kollegen\", \"kumpanen\", \"besucher\"]\n",
    "\n",
    "corpus = \"all_corpora_9522_targets.preprocessed2\" \n",
    "fout = \"ngrams.tsv\"\n",
    "\n",
    "# Define a function to find all the words that co-occur with the keywords in each line\n",
    "def find_co_occurrences(keywords, text):\n",
    "    result = []\n",
    "\n",
    "    for line in text.split('\\n'):\n",
    "        line_co_occurrences = {}\n",
    "\n",
    "        for keyword in keywords:\n",
    "            if keyword in line:\n",
    "                line_co_occurrences[keyword] = []\n",
    "\n",
    "                # Look for words from the line, excluding the keyword\n",
    "                words = [word.strip('.,?!') for word in line.split() if word.lower() != keyword.lower()]\n",
    "                line_co_occurrences[keyword] = words\n",
    "\n",
    "        result.append(line_co_occurrences)\n",
    "\n",
    "    return result\n",
    "\n",
    "# A function to count the co-occurrences\n",
    "def count_co_occurrences(co_occurrences):\n",
    "    counts = {}\n",
    "\n",
    "    for line_co_occurrences in co_occurrences:\n",
    "        for keyword, words in line_co_occurrences.items():\n",
    "            for word in words:\n",
    "                key = (keyword, word)\n",
    "                counts[key] = counts.get(key, 0) + 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "def filter_counts(counts, keywords):\n",
    "    filtered_counts = {key: count for key, count in counts.items() if key[1].lower() not in map(str.lower, keywords)}\n",
    "    return filtered_counts\n",
    "\n",
    "\n",
    "def write_to_file(filtered_counts, output_filename):\n",
    "    with open(output_filename, 'w') as f:\n",
    "        f.write(\"Keyword\\tCo-Word\\tCount\\n\")\n",
    "\n",
    "        for (keyword, word), count in sorted(filtered_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            f.write(f\"{keyword}\\t{word}\\t{count}\\n\")\n",
    "\n",
    "with open(corpus, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "co_occurrences = find_co_occurrences(keywords, text)\n",
    "counts = count_co_occurrences(co_occurrences)\n",
    "\n",
    "# Remove rows where the words in the second column contain any of the keywords from the first column\n",
    "filtered_counts = {key: count for key, count in counts.items() if all(keyword.lower() not in key[1].lower() for keyword in keywords)}\n",
    "\n",
    "write_to_file(filtered_counts, fout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "7e2e04f3-fee7-48d9-aa55-f630b5ea9736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter keyword:  besucher\n",
      "Enter the co-occurring word:  eintritt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword         Co-Word         Count     \n",
      "========================================\n",
      "Besucher        Eintritt        403       \n",
      "Besucher        Eintrittskarte  65        \n",
      "Besucher        Eintrittskarten 63        \n",
      "Besucher        Eintrittspreis  36        \n",
      "Besucher        Eintrittspreise 35        \n",
      "Besucher        Eintrittsgeld   22        \n",
      "Besucher        Eintrittsgelder 12        \n",
      "Besucher        Eintrittsgeldern 11        \n",
      "Besucher        eintritt        7         \n",
      "Besucher        Eintrittspreisen 6         \n",
      "Besucher        Eintritte       5         \n",
      "Besucher        Eintrittspreises 5         \n",
      "Besucher        Eintrittsgebühren 5         \n",
      "Besucher        Eintritten      4         \n",
      "Besucher        Eintrittsgebühr 4         \n",
      "Besucher        Eintrittsticket 3         \n",
      "Besucher        Eintrittsbändchen 3         \n",
      "Besucher        Eintritts       3         \n",
      "Besucher        eintrittspflichtigen 2         \n",
      "Besucher        Eintrittsarmband 2         \n",
      "Besucher        Gratiseintritt  2         \n",
      "Besucher        Parkeintritt    2         \n",
      "Besucher        Eintrittsgeldes 2         \n",
      "Besucher        Eintrittsverbot 1         \n",
      "Besucher        Eintrittsbändel 1         \n",
      "Besucher        Mehrtageskarteneintritte 1         \n",
      "Besucher        ILAEintrittskarte 1         \n",
      "Besucher        eintrittsfrei   1         \n",
      "Besucher        Mehrfacheintritte 1         \n",
      "Besucher        Eintrittsabteilung 1         \n",
      "Besucher        MesseEintrittskarte 1         \n",
      "Besucher        ZusatzEintritt  1         \n",
      "Besucher        Ersteintritte   1         \n",
      "Besucher        Eintrittssystem 1         \n",
      "Besucher        EIntrittspreis  1         \n",
      "Besucher        Eintrittstickets 1         \n",
      "Besucher        Gratiseintritts 1         \n",
      "Besucher        Tageseintrittspreis 1         \n",
      "Besucher        eintrittsfreien 1         \n",
      "Besucher        Tageseintritt   1         \n",
      "Besucher        EintrittsCode   1         \n",
      "Besucher        KinoEintritt    1         \n",
      "Besucher        GPLEintrittskarte 1         \n",
      "Besucher        Eintrittserlösen 1         \n",
      "Besucher        LabyrinthEintrittskarte 1         \n",
      "Besucher        Eintrittsarmbändchen 1         \n",
      "Besucher        Eintrittsbändeli 1         \n",
      "Besucher        Eintrittserlöse 1         \n",
      "Besucher        Eintrittsnachweis 1         \n",
      "Besucher        Eintrittpreisen 1         \n",
      "Besucher        EintrittBezahler 1         \n",
      "Besucher        Museumseintritt 1         \n",
      "========================================\n",
      "Total Count: 731\n"
     ]
    }
   ],
   "source": [
    "# Look up the keyword and print the rows that contain the input keyword\n",
    "\n",
    "\n",
    "output_filename = \"ngrams.tsv\"\n",
    "output_filtered_filename = \"lookup.tsv\"\n",
    "\n",
    "\n",
    "def filter_table(rows, input_keywords):\n",
    "    filtered_rows = [row for row in rows if all(keyword.lower() in row[0].lower() and other_word.lower() in row[1].lower() for keyword, other_word in input_keywords)]\n",
    "    return filtered_rows\n",
    "\n",
    "def save_filtered_rows_to_file(filtered_rows, output_filename):\n",
    "    with open(output_filename, 'w') as f:\n",
    "        f.write(\"Keyword\\tCo-Word\\tCount\\n\")\n",
    "        for row in filtered_rows:\n",
    "            f.write(f\"{row[0]}\\t{row[1]}\\t{row[2]}\\n\")\n",
    "\n",
    "def print_filtered_rows(filtered_rows):\n",
    "    if not filtered_rows:\n",
    "        print(\"No matching rows found.\")\n",
    "        return\n",
    "\n",
    "    print(\"{:<15} {:<15} {:<10}\".format(\"Keyword\", \"Co-Word\", \"Count\"))\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    total_count = 0\n",
    "    for row in filtered_rows:\n",
    "        print(\"{:<15} {:<15} {:<10}\".format(row[0], row[1], row[2]))\n",
    "        total_count += row[2]\n",
    "\n",
    "    print(\"=\"*40)\n",
    "    print(\"Total Count: {}\".format(total_count))\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'output.tsv' is the generated output file from your previous script\n",
    "\n",
    "keyword_input = input(\"Enter keyword: \")\n",
    "other_word_input = input(\"Enter the co-occurring word: \")\n",
    "input_keywords = [(keyword_input, other_word_input)]\n",
    "\n",
    "\n",
    "# Read data from the output file\n",
    "rows = []\n",
    "with open(output_filename, 'r') as f:\n",
    "    next(f)  # skip header\n",
    "    for line in f:\n",
    "        keyword, co_word, count = line.strip().split('\\t')\n",
    "        rows.append((keyword, co_word, int(count)))\n",
    "\n",
    "# Filter and save rows based on user input\n",
    "filtered_rows = filter_table(rows, input_keywords)\n",
    "save_filtered_rows_to_file(filtered_rows, output_filtered_filename)\n",
    "\n",
    "# Print the filtered rows and total count\n",
    "print_filtered_rows(filtered_rows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "bd5d1aa2-3f59-4f94-b27b-3d71e76f114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the keyword:  Propheten\n",
      "Enter the co-occurring word:  nachbar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 11233: Tour führt meist abseits großer Straßen Grünen Prophetensee benachbarten Spielplatz Quickborn\n",
      "Line 25254: gigantische zehn Quadratkilometer große Friedhof Nadschaf letzte Ruhestätte zwei Millionen Muslimen Welt benachbarte Moschee Grablege Imams Schwiegersohn Propheten Mohammed\n",
      "Line 440460: Haiy unverhüllte Wahrheit erkannt Menschen Nachbarinsel Propheten Anweisungen Symbole verkündet wurde\n",
      "Line 775084: Imam Gemeinde Samir Haskic betonte Bedeutung guter Nachbarschaft anonymisierten schnelllebigen Gesellschaft verwies Lehren Propheten Mohammed\n",
      "Line 841668: Assauer erwartet Entwicklung beim Nachbarn Dortmund erklärt eigenwilligen Diktion unerwarteten Pokalsieg wirtschaftlich Arsch Propheten\n",
      "Line 859829: Ende Anfang benachbaren Kopenhagen geklungen nachdem JyllandsPosten zwölf Karikaturen islamischen Propheten veröffentlicht\n",
      "Line 908799: magische Nacht bevorsteht Ankunft Propheten gefeiert gibt Zehra Sohn Großmutter Nachbardorf\n"
     ]
    }
   ],
   "source": [
    "# A quick check if these lines actually contain both words\n",
    "file_path = \"all_corpora_9522_targets.preprocessed2\"\n",
    "\n",
    "def print_lines_with_keywords(file_path, keyword1, word2):\n",
    "    keyword1_lower = keyword1.lower()\n",
    "    word2_lower = word2.lower()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line_num, line in enumerate(file, start=1):\n",
    "            if keyword1_lower in line.lower() and word2_lower in line.lower():\n",
    "                print(f\"Line {line_num}: {line.strip()}\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "keyword1 = input(\"Enter the keyword: \")\n",
    "word2 = input(\"Enter the co-occurring word: \")\n",
    "\n",
    "print_lines_with_keywords(file_path, keyword1, word2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
